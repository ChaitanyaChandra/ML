{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13360758,"sourceType":"datasetVersion","datasetId":7769196}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/chaitanyachandra/08-ml-r-squared-regression-model-selection?scriptVersionId=268396861\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# RÂ² (R-squared) â€” Coefficient of Determination\n\n\n\n## Concept\n\n**Goal:**\nTo measure **how well a regression model fits** the actual data â€” i.e., how much of the variation in the target variable $( y )$ is explained by your model.\n\n\n\n## Explanation\n\n### 1. Two Situations to Compare\n\n\n1. **Regression line** â†’ a line fitted by the model (using ordinary least squares).\n2. **Average line** â†’ a horizontal line at the mean of all $( y )$-values.\n\n\n\n### 2. Residual Sum of Squares (SSR or RSS)\n\nWhen you draw the **regression line**, you measure how far each real data point $( y_i )$ is from the predicted point $( \\hat{y}_i )$ on the line.\n\n$SS_{res} = \\sum (y_i - \\hat{y}_i)^2$\n\nâœ… This tells you how much **error** (unexplained variance) your model still has after fitting.\n\n\n\n### 3. Total Sum of Squares (SST)\n\nNow imagine no model at all â€” just an **average line** through the mean of all $( y )$-values $(( \\bar{y} ))$.\n\n$\nSS_{tot} = \\sum (y_i - \\bar{y})^2\n$\n\nâœ… This shows **total variation** in the data â€” how much all $( y_i )$ values differ from their average.\n\n\n\n### 4. Formula for RÂ²\n\n$\nR^2 = 1 - \\frac{SS_{res}}{SS_{tot}}\n$\n\n\n\n### 5. Intuition\n\n* If your regression line fits well â†’ $( SS_{res} )$ is **small**, so $( R^2 )$ is **close to 1** âœ…\n* If your regression line fits poorly â†’ $( SS_{res} )$ is **large**, so $( R^2 )$ is **close to 0** âŒ\n\n\n\n### 6. Typical RÂ² Values and Interpretation\n\n| RÂ² Value      | Interpretation                                                                     |\n| - | - |\n| **1.0**       | Perfect fit â€” model predicts every point exactly (almost impossible in real data). |\n| **0.9**       | Excellent fit â€” model explains 90% of data variation.                              |\n| **0.7 â€“ 0.9** | Good fit â€” quite reliable.                                                         |\n| **0.4 â€“ 0.7** | Weak fit â€” model misses a lot of variation.                                        |\n| **< 0.4**     | Poor fit â€” not a good model.                                                       |\n| **< 0**       | Model is worse than just using the mean â€” model is *nonsense* for this data.       |\n\n\n\n### 8. Summary\n\n> RÂ² tells you **how much better your model is than just predicting the average** every time.\n\nIf your model doesnâ€™t improve much over just using the mean, RÂ² will be low.\nIf it explains most of the variation, RÂ² will be high.\n\n---\n\n## Adjusted RÂ² (Adjusted R Squared)\n\n### Definition\n\n**Adjusted RÂ²** is a modified version of **RÂ² (Coefficient of Determination)** that adjusts for the number of independent variables in a regression model.\nIt penalizes the model for adding unnecessary variables that do not significantly improve the prediction.\n\n\n\n## âš ï¸ Problem with RÂ²\n\nWhen you **add more variables** (Xâ‚ƒ, Xâ‚„, etc.) to a regression model:\n\n* The **total sum of squares (SSâ‚œâ‚’â‚œ)** remains the same (it depends only on actual y values).\n* The **residual sum of squares (SSáµ£â‚‘â‚›)** **can only decrease or stay the same** because:\n\n  * The **Ordinary Least Squares (OLS)** method minimizes SSáµ£â‚‘â‚›.\n  * If the new variable helps, SSáµ£â‚‘â‚› decreases.\n  * If it doesnâ€™t help, OLS sets its coefficient (bâ‚ƒ) to **zero**, keeping SSáµ£â‚‘â‚› unchanged.\n* Therefore, **RÂ² never decreases**, even if the new variable is useless.\n\nğŸ§  **Result:**\nYou might end up with a model that includes unnecessary variables just because RÂ² keeps increasing.\n\n\n## ğŸ’¡ Solution: Adjusted RÂ²\n\n### Formula\n\n$\n\\text{Adjusted } R^2 = 1 - \\left( \\frac{(1 - R^2)(n - 1)}{n - k - 1} \\right)\n$\n\nWhere:\n\n* **n** = Number of observations (sample size)\n* **k** = Number of independent variables\n\n\n### How It Works\n\n* As **k (number of predictors)** increases, the **denominator** ((n - k - 1)) decreases.\n* This makes the ratio larger â†’ leading to **lower Adjusted RÂ²**.\n* Therefore, **Adjusted RÂ² penalizes** adding variables that donâ€™t significantly improve the model.\n\n\n### Key Insight\n\n* Adding a new variable will **only increase Adjusted RÂ²** if the variable provides a **meaningful improvement** in prediction.\n* If it doesnâ€™t, Adjusted RÂ² will **decrease** â€” discouraging overfitting.\n\n\n##  Summary\n\n| Concept           | Description                                                                                                |\n| ----------------- | ---------------------------------------------------------------------------------------------------------- |\n| **RÂ²**            | Measures model fit â€” how much of yâ€™s variation is explained by xâ€™s.                                        |\n| **Issue with RÂ²** | Always increases when new variables are added, even useless ones.                                          |\n| **Adjusted RÂ²**   | Adds a penalty for adding variables â€” increases only if the new variable significantly improves model fit. |\n| **Purpose**       | Helps build parsimonious (simple but effective) models.                                                    |\n\n\n## Intuitive Understanding\n\n* RÂ² = â€œHow well are we fitting the data?â€\n* Adjusted RÂ² = â€œHow well are we fitting the data **without overcomplicating** the model?â€\n\n---\n\n\n### **Solve:**\n\nYou are given an [Energy.csv dataset](https://www.kaggle.com/datasets/chaitanyachandra/data-csv?select=Energy.csv)  dataset containing several independent features (e.g., temperature, pressure, etc.) and one dependent variable (Energy Output).\n\nYour task is to compare different regression models and evaluate their performance.\n\n\n### **Tasks:**\n\n1. **Load and explore** the dataset from [Energy.csv dataset](https://www.kaggle.com/datasets/chaitanyachandra/data-csv?select=Energy.csv).\n2. **Split** the dataset into training and testing sets (80% train, 20% test).\n3. Train the following regression models:\n\n   * **Linear Regression**\n   * **Polynomial Regression (degree = 4)**\n   * **Support Vector Regression (RBF kernel)**\n   * **Decision Tree Regression**\n   * **Random Forest Regression (n_estimators = 100)**\n4. For **Support Vector Regression**, make sure to:\n   * Apply **feature scaling** to both the independent and dependent variables.\n   * **Inverse transform** predictions to their original scale.\n5. Compute the **RÂ² score** for each model and store the results in a dictionary named `scores`.\n6. **Plot a bar chart** comparing the RÂ² scores of all models.\n\n\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"# import packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import r2_score\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.svm import SVR\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.tree import  DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# variables\nscores = {}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# read CSV\ndata = pd.read_csv(\"/kaggle/input/data-csv/Energy.csv\")\ndata.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# split dependent and independent cells\nindependent_x = data.iloc[:, :-1].values\ndependent_y = data.iloc[:, -1].values","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# split train and test data \nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(independent_x, dependent_y, random_state=0, test_size=0.2)\n\nx_train, x_test, y_train, y_test","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Multi-Linear regression \nlr = LinearRegression()\nlr.fit(x_train, y_train)\nscores[\"Linear\"] = r2_score(y_test, lr.predict(x_test))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Polynomial Linear Regression\npr = PolynomialFeatures(degree=4)\nlr2 = LinearRegression()\nlr2.fit(pr.fit_transform(x_train), y_train)\nscores[\"Polynomial\"] = r2_score(y_test, lr2.predict(pr.transform(x_test)))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Support Vector Regression\nsc_x = StandardScaler()\nsc_y = StandardScaler()\n\nx_train_sc = sc_x.fit_transform(x_train)                   \nx_test_sc  = sc_x.transform(x_test)                      \n\ny_train_2d = np.asarray(y_train).reshape(-1, 1)             \ny_train_sc = sc_y.fit_transform(y_train_2d).ravel()      \n\nsvm = SVR(kernel='rbf')                                     \nsvm.fit(x_train_sc, y_train_sc)                             \n\ny_pred_sc = svm.predict(x_test_sc)                          \ny_pred = sc_y.inverse_transform(y_pred_sc.reshape(-1,1)).ravel()  \n\nscores[\"Support Vector\"] = r2_score(y_test, y_pred) ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Decision Tree Regression\ndt = DecisionTreeRegressor()\ndt.fit(x_train, y_train)\nscores[\"Decision Tree\"] = r2_score(y_test, dt.predict(x_test)) ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Random Forest Regression\nrfr = RandomForestRegressor(random_state=0, n_estimators=10)\nrfr.fit(x_train, y_train)\nscores[\"Random Forest\"] = r2_score(y_test, rfr.predict(x_test)) ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"scores","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"models = list(scores.keys())\nvalues = list(scores.values())\n\n# Plot\nplt.figure(figsize=(8,5))\nplt.bar(models, values, color='skyblue')\nplt.title('Model Performance Comparison')\nplt.ylabel('RÂ² Score')\nplt.xticks(rotation=20, ha='right')\nplt.ylim(0.9, 1)\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}